{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f0addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython\n",
    "import keras\n",
    "import librosa\n",
    "import whisper\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from translate import Translator\n",
    "import openai as analizer\n",
    "\n",
    "translator= Translator(from_lang=\"es\",to_lang=\"en\")\n",
    "\n",
    "modelo = whisper.load_model(\"medium\")\n",
    "\n",
    "class Predictions:\n",
    "\n",
    "    def __init__(self, path, file):\n",
    "        self.path = path\n",
    "        self.file = file\n",
    "    def cargar_modelo(self):\n",
    "        self.cargar_modelo = keras.models.load_model(self.path)\n",
    "        return self.cargar_modelo.summary()\n",
    "    def predicciones(self):\n",
    "        data, sampling_rate = librosa.load(self.file)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=40).T, axis=0)\n",
    "        x = np.expand_dims(mfccs, axis=1)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        predictions = np.argmax(self.cargar_modelo.predict(x),axis=-1)\n",
    "        print(\"Emocion identificada:\", self.convertclasstoemotion(predictions))    \n",
    "    @staticmethod\n",
    "    def convertclasstoemotion(pred):\n",
    "     \n",
    "        label_conversion = {'0': 'triste',\n",
    "                            '1': 'feliz',\n",
    "                            '2': 'neutral',\n",
    "                         }\n",
    "        for key, value in label_conversion.items():\n",
    "            if int(key) == pred:\n",
    "                label = value\n",
    "                global emotion\n",
    "                emotion=value\n",
    "        return label\n",
    "\n",
    "pred = Predictions(path='trainedModel/emotion.h5',file='noeneutral.wav')\n",
    "pred2= Predictions(path='trainedModel/emotion.h5',file='noefeliz.wav')\n",
    "pred3= Predictions(path='trainedModel/emotion.h5',file='maria2.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0461f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               10496     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,474\n",
      "Trainable params: 110,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               10496     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,474\n",
      "Trainable params: 110,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               10496     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,474\n",
      "Trainable params: 110,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noemisael/.local/lib/python3.8/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load audio: ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libavresample   4.  0.  0 /  4.  0.  0\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\nnoeneutral.wav: No such file or directory\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/whisper/audio.py:46\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# This launches a subprocess to decode audio while down-mixing and resampling as necessary.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 46\u001b[0m         \u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms16le\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macodec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpcm_s16le\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mffmpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-nostdin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_stdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_stderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ffmpeg\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ffmpeg/_run.py:325\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(stream_spec, cmd, capture_stdout, capture_stderr, input, quiet, overwrite_output)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, out, err)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, err\n",
      "\u001b[0;31mError\u001b[0m: ffmpeg error (see stderr output for detail)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pred2\u001b[38;5;241m.\u001b[39mcargar_modelo()\n\u001b[1;32m      3\u001b[0m pred3\u001b[38;5;241m.\u001b[39mcargar_modelo()\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodelo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnoeneutral.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m texto\u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m result2 \u001b[38;5;241m=\u001b[39m modelo\u001b[38;5;241m.\u001b[39mtranscribe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoefeliz.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/whisper/transcribe.py:121\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[1;32m    118\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decode_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/whisper/audio.py:130\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(audio):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m         audio \u001b[38;5;241m=\u001b[39m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(audio)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/whisper/audio.py:51\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     45\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     46\u001b[0m         ffmpeg\u001b[38;5;241m.\u001b[39minput(file, threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;241m.\u001b[39moutput(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms16le\u001b[39m\u001b[38;5;124m\"\u001b[39m, acodec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpcm_s16le\u001b[39m\u001b[38;5;124m\"\u001b[39m, ac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ar\u001b[38;5;241m=\u001b[39msr)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;241m.\u001b[39mrun(cmd\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-nostdin\u001b[39m\u001b[38;5;124m\"\u001b[39m], capture_stdout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, capture_stderr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ffmpeg\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfrombuffer(out, np\u001b[38;5;241m.\u001b[39mint16)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32768.0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load audio: ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libavresample   4.  0.  0 /  4.  0.  0\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\nnoeneutral.wav: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "pred.cargar_modelo()\n",
    "pred2.cargar_modelo()\n",
    "pred3.cargar_modelo()\n",
    "    \n",
    "result = modelo.transcribe(\"noeneutral.wav\")\n",
    "texto= result[\"text\"]\n",
    "result2 = modelo.transcribe(\"noefeliz.wav\")\n",
    "texto2=result2[\"text\"]\n",
    "result3 = modelo.transcribe(\"maria2.wav\")\n",
    "texto3=result3[\"text\"]\n",
    "ingles = translation = translator.translate(texto)\n",
    "ingles2= translation = translator.translate(texto2)\n",
    "ingles3= translation = translator.translate(texto3)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia2= SentimentIntensityAnalyzer()\n",
    "sia3= SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(ingles)\n",
    "sia2.polarity_scores(ingles2)\n",
    "sia3.polarity_scores(ingles3)\n",
    "\n",
    "ltk= sia.polarity_scores(ingles)\n",
    "ltk2= sia2.polarity_scores(ingles2)\n",
    "ltk3= sia3.polarity_scores(ingles3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('noeneutral.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('noefeliz.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0da4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio('maria2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" En este apartado se imprimen los resultados obtenidos\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"prueba 1 \\n\")\n",
    "\n",
    "pred.predicciones()\n",
    "\n",
    "\n",
    "emocion_detectada = \"Recomendaciones si me siento\" + emotion\n",
    "analizer.api_key = \"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\"\n",
    "completion = analizer.Completion.create(engine=\"text-davinci-003\",\n",
    "                                      prompt=emocion_detectada,\n",
    "                                      max_tokens=2048)\n",
    "\n",
    "print(\"\\nSegún las palabras detectadas en el audio fueron:\\n\")\n",
    "print(texto +\"\\n\")\n",
    "\n",
    "\n",
    "if(list(ltk.values())[0] > list(ltk.values())[1] and list(ltk.values())[0] > list(ltk.values())[2]):\n",
    "    print(\"Emocion detectada triste\\n\")\n",
    "elif(list(ltk.values())[1] > list(ltk.values())[0] and list(ltk.values())[1] > list(ltk.values())[2]):\n",
    "    print(\"Emocion detectada neutra\\n\")\n",
    "else:\n",
    "    print(\"Emocion detectada feliz\\n\")\n",
    "    \n",
    "\n",
    "print(\"Las recomendaciones para el estado de animo presentado son los siguientes:\")\n",
    "print(completion.choices[0].text)\n",
    "\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(\"\\n\\nprueba 2 \\n\")\n",
    "pred2.predicciones()\n",
    "\n",
    "\n",
    "emocion_detectada = \"Recomendaciones si me siento\" + emotion\n",
    "analizer.api_key = \"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\"\n",
    "completion = analizer.Completion.create(engine=\"text-davinci-003\",\n",
    "                                      prompt=emocion_detectada,\n",
    "                                      max_tokens=2048)\n",
    "\n",
    "print(\"\\nSegún las palabras detectadas en el audio fueron:\\n\")\n",
    "print(texto2 +\"\\n\")\n",
    "\n",
    "\n",
    "if(list(ltk2.values())[0] > list(ltk2.values())[1] and list(ltk2.values())[0] > list(ltk2.values())[2]):\n",
    "    print(\"Emocion detectada triste\\n\")\n",
    "elif(list(ltk2.values())[1] > list(ltk2.values())[0] and list(ltk2.values())[1] > list(ltk2.values())[2]):\n",
    "    print(\"Emocion detectada neutra\\n\")\n",
    "else:\n",
    "    print(\"Emocion detectada feliz\\n\")\n",
    "    \n",
    "\n",
    "print(\"Las recomendaciones para el estado de animo presentado son los siguientes:\")\n",
    "print(completion.choices[0].text)\n",
    "print(\"_______________________________________________________________________________\")\n",
    "\n",
    "print(\"\\n\\nprueba 3 \\n\")\n",
    "\n",
    "pred3.predicciones()\n",
    "\n",
    "\n",
    "emocion_detectada = \"Recomendaciones si me siento\" + emotion\n",
    "analizer.api_key = \"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\"\n",
    "completion = analizer.Completion.create(engine=\"text-davinci-003\",\n",
    "                                      prompt=emocion_detectada,\n",
    "                                      max_tokens=2048)\n",
    "\n",
    "print(\"\\nSegún las palabras detectadas en el audio fueron:\\n\")\n",
    "print(texto3 +\"\\n\")\n",
    "\n",
    "\n",
    "if(list(ltk3.values())[0] > list(ltk3.values())[1] and list(ltk3.values())[0] > list(ltk3.values())[2]):\n",
    "    print(\"Emocion detectada triste\\n\")\n",
    "elif(list(ltk3.values())[1] > list(ltk3.values())[0] and list(ltk3.values())[1] > list(ltk3.values())[2]):\n",
    "    print(\"Emocion detectada neutra\\n\")\n",
    "else:\n",
    "    print(\"Emocion detectada feliz\\n\")\n",
    "    \n",
    "\n",
    "print(\"Las recomendaciones para el estado de animo presentado son los siguientes:\")\n",
    "print(completion.choices[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiomodel",
   "language": "python",
   "name": "audiomodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
